{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06aad8f1",
   "metadata": {},
   "source": [
    "# Cloud Data Transfer Speeds Benchmarking Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551822",
   "metadata": {},
   "source": [
    "Add overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52c47",
   "metadata": {},
   "source": [
    "## Step 0: Load Required Setup Packages & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e427e0",
   "metadata": {},
   "source": [
    "Installs required workflow setup packages and calls UI generation script. If one or more of the packages don't exist in your `base` environment, they will install for you. Note that if installation is required, this cell will take a few minutes to complete execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73185f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking conda environment for UI depedencies...\n",
      "All dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "print('Checking conda environment for UI depedencies...')\n",
    "os.system(\"bash \" + os.getcwd() + \"/jupyter-helpers/install_ui_packages.sh\")\n",
    "print('All dependencies installed.')\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + '/jupyter-helpers')\n",
    "import ui_helpers as ui\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be35fd4",
   "metadata": {},
   "source": [
    "## Step 1: Define Workflow Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de7e39",
   "metadata": {},
   "source": [
    "Run the following cells to generate interactive widgets allowing you to enter all workflow inputs. **All inputs must be filled out to proceed with the benchmarking process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cb664",
   "metadata": {},
   "source": [
    "### Cloud Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7e51",
   "metadata": {},
   "source": [
    "#### Compute Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d055",
   "metadata": {},
   "source": [
    "Before defining anything else, the resources you intend to use with the benchmarking must be defined. Currently, only resources defined in the Parallel Works platform may be used. Also of note are options that will be passed to Dask: you have full control over how many cores and memory you want each worker from your cluster to use, as well as how many nodes you want to be active at a single time.\n",
    "\n",
    "In particular, these options are included so that you can form fair comparisons between different cloud service providers (CSPs). Generally, different CSPs won't have worker nodes with the exact same specs, and in order to achieve a fair comparison between two CSPs one cluster will have to limited to not exceed to the computational power of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376e50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For resources controlled from the Parallel Works platform, the <code>Resource name</code> box should be populated with the name found on the <b>RESOURCES</b>  tab.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21753c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resource = ui.resourceWidgets()\n",
    "resource.display()\n",
    "resources = resource.processInput()\n",
    "print(f'Your resource inputs:\\n {resources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4fe8c",
   "metadata": {},
   "source": [
    "#### Object Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea7fb6",
   "metadata": {},
   "source": [
    "This set of inputs is where you enter the cloud object store Universal Resource Identifiers (URIs). Both public and private buckets are supported. For the latter, ensure that you have access credentials with *at least* read, write, list, and put (copy from local storage to cloud) permissions, as format conversions will need to be made during the benchmarking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b144152",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = ui.storageWidgets()\n",
    "store.display()\n",
    "storage = store.processInput()\n",
    "print(f'Your storage inputs:\\n {storage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711e2b",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323384",
   "metadata": {},
   "source": [
    "#### User-Supplied Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bd7e0",
   "metadata": {},
   "source": [
    "Below you can specify datasets that you want to be tested in the benchmarking. You can either enter single files or multiple files that belong to a single dataset, but that dataset match at least one of the supported formats. **Read the following input rules after running the UI cell below this one.**\n",
    "\n",
    "\n",
    "1. Activate the checkbox if you desire to record your user-defined datasets. If it is not checked, none of your inputs will be recorded.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Input the full URI or absolute path of the data location (`<URI prefix>://bucket-name/path/to/file.extension` or `/path/to/file.extension`)\n",
    "    - Use globstrings (`<URI prefix>://bucket-name/path/to/files/*` or `path/to/files/*`) to specify datasets that are split up into multiple subfiles.\n",
    "    - If using a globstring, ensure that *only* files that belong to the dataset exist in that directory. The workflow will take all files in the directory before the `*` and attempt to gather them into a single dataset.\n",
    "    \n",
    "<br>\n",
    "\n",
    "3. If you have a dataset stored in multiple cloud storage locations that will be used in the benchmarking, you must input the full URI of that dataset for each of these locations. That is, you must define each location of the data separately.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. For single-file datasets > 5.4 GB (5 GiB) in size that may need to be transferred across clouds in the benchmarking, you must transfer these files to the desired locations *before* running the benchmarking.\n",
    "    - This is because `gsutil`, one of the tools the benchmark uses to copy user-defined files from the original cloud storage location to other benchmarking locations, can only handle single-file transfers between CSPs that are smaller than 5 GiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "userdata = ui.userdataWidgets(storage=storage)\n",
    "userdata.display()\n",
    "user_files = userdata.processInput()\n",
    "print(f'Your data inputs:\\n {user_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c5eb",
   "metadata": {},
   "source": [
    "#### Randomly-Generated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322692c1",
   "metadata": {},
   "source": [
    "Another option to supply data to the benchmarking is to create randomly-generated datasets. These sets can be as large as you want (as they are written in parallel), and provide a great option if you are new to the world of cloud-native data formats. There are currently two supported randomly-generated data formats: CSV and NetCDF4. Since NetCDF4 is a gridded data format, an option to specify the number of coordinate axes is also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46825cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "randgen = ui.randgenWidgets(resources=resources)\n",
    "randgen.display()\n",
    "randfiles = randgen.processInput()\n",
    "print(f'Your randomly-generated file options:\\n {randfiles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc612fa1",
   "metadata": {},
   "source": [
    "### TODO: Cloud-Native Format Conversion Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581d0ee",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d585b",
   "metadata": {},
   "source": [
    "## Step 2: Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f2c55",
   "metadata": {},
   "source": [
    "Executing the following cell will write all of your inputs to `inputs.json`, install miniconda3 and the \"cloud-data\" Python environment to all resources, and write randomly-generated files to all cloud storage locations (if any files were specified). If writing randomly-generated files, especially large ones, the execution of this cell may take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb642c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "While randomly-generated files are written in parallel by default, if you wish to speed up the execution of this cell, consider creating/choosing a resource with more powerful worker nodes.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff33e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Setting up workflow...')\n",
    "\n",
    "user_input = json.dumps({\"RESOURCES\" : resources,\n",
    "                         \"STORAGE\" : storage,\n",
    "                         \"USERFILES\" : user_files,\n",
    "                         \"RANDFILES\" : randfiles\n",
    "                        })\n",
    "\n",
    "with open('inputs.json', 'w') as outfile:\n",
    "    outfile.write(user_input)\n",
    "\n",
    "os.system(\"bash workflow_notebook_setup.sh\")\n",
    "\n",
    "print('Workflow setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ea9b",
   "metadata": {},
   "source": [
    "## Step 3: Run Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d260",
   "metadata": {},
   "source": [
    "### Convert File to Cloud-Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f82bb8",
   "metadata": {},
   "source": [
    "Since one of the major goals of this benchmarking is testing legacy formats against cloud-native ones, we must convert your legacy-formatted data (CSV and NetCDF4) into their corresponding cloud-native formats. This cell will execute and time the conversion process, writing each new format in parallel. The conversion will be done using each cluster's full amount of resources, so be mindful of this feature when using clusters that are expensive to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4685034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for worker nodes to start up...\n",
      "Workers active.\n",
      "Converting files in \"gs://cloud-data-benchmarks\" with \"gcptestnew\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jgreen/cloud-data-transfer-benchmarking/benchmarks-core/convert-data.py\", line 181, in <module>\n",
      "    ds = core.combine_nc_subfiles(base_uri, filename, storage_options, fs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'core_helpers' has no attribute 'combine_nc_subfiles'\n",
      "cloud-data-transfer-benchmarking/outputs/results_tmp.csv: No such file or directory\n",
      "rm: cannot remove ‘results_tmp.csv’: No such file or directory\n",
      "cat: /home/jgreen/pw/storage/cloud-data-transfer-benchmarking/results/csv-files/results_tmp.csv: No such file or directory\n",
      "rm: cannot remove ‘/home/jgreen/pw/storage/cloud-data-transfer-benchmarking/results/csv-files/results_tmp.csv’: No such file or directory\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbash benchmarks-core/run_benchmark_step.sh \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mconvert-data.py\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mconversions.csv\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m df_conversions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/results/csv-files/conversions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_conversions\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1753\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:79\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3c/envs/jupyter/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:554\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"convert-data.py\\\" \\\"conversions.csv\\\"\")\n",
    "df_conversions = pd.read_csv(os.getcwd() + '/results/csv-files/conversions.csv')\n",
    "df_conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702d935",
   "metadata": {},
   "source": [
    "### File Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f2e3f",
   "metadata": {},
   "source": [
    "The last computation-intensive test in the benchmarking is reading and timing files from cloud storage. This will give you an idea of what data transfer throughput you can expect when using cloud storage and different data formats in other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"read-data.py\\\" \\\"reads.csv\\\"\")\n",
    "df_reads = pd.read_csv(os.getcwd() + '/results/csv-files/reads.csv')\n",
    "df_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a169d8",
   "metadata": {},
   "source": [
    "## TODO: Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d69914",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
