{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06aad8f1",
   "metadata": {},
   "source": [
    "# Cloud Data Transfer Speeds Benchmarking Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551822",
   "metadata": {},
   "source": [
    "Add overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52c47",
   "metadata": {},
   "source": [
    "## Step 0: Load Required Setup Packages & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e427e0",
   "metadata": {},
   "source": [
    "Installs required workflow setup packages and calls UI generation script. If one or more of the packages don't exist in your `base` environment, they will install for you. Note that if installation is required, this cell will take a few minutes to complete execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73185f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking conda environment for UI depedencies...\n",
      "All dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "print('Checking conda environment for UI depedencies...')\n",
    "os.system(\"bash \" + os.getcwd() + \"/jupyter-helpers/install_ui_packages.sh\")\n",
    "print('All dependencies installed.')\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + '/jupyter-helpers')\n",
    "import ui_helpers as ui\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be35fd4",
   "metadata": {},
   "source": [
    "## Step 1: Define Workflow Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de7e39",
   "metadata": {},
   "source": [
    "Run the following cells to generate interactive widgets allowing you to enter all workflow inputs. **All inputs must be filled out to proceed with the benchmarking process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cb664",
   "metadata": {},
   "source": [
    "### Cloud Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7e51",
   "metadata": {},
   "source": [
    "#### Compute Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d055",
   "metadata": {},
   "source": [
    "Before defining anything else, the resources you intend to use with the benchmarking must be defined. Currently, only resources defined in the Parallel Works platform may be used. Also of note are options that will be passed to Dask: you have full control over how many cores and memory you want each worker from your cluster to use, as well as how many nodes you want to be active at a single time.\n",
    "\n",
    "In particular, these options are included so that you can form fair comparisons between different cloud service providers (CSPs). Generally, different CSPs won't have worker nodes with the exact same specs, and in order to achieve a fair comparison between two CSPs one cluster will have to limited to not exceed to the computational power of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376e50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For resources controlled from the Parallel Works platform, the <code>Resource name</code> box should be populated with the name found on the <b>RESOURCES</b>  tab.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21753c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resource = ui.resourceWidgets()\n",
    "resource.display()\n",
    "resources = resource.processInput()\n",
    "print(f'Your resource inputs:\\n {resources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4fe8c",
   "metadata": {},
   "source": [
    "#### Object Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea7fb6",
   "metadata": {},
   "source": [
    "This set of inputs is where you enter the cloud object store Universal Resource Identifiers (URIs). Both public and private buckets are supported. For the latter, ensure that you have access credentials with *at least* read, write, list, and put (copy from local storage to cloud) permissions, as format conversions will need to be made during the benchmarking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b144152",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = ui.storageWidgets()\n",
    "store.display()\n",
    "storage = store.processInput()\n",
    "print(f'Your storage inputs:\\n {storage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711e2b",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323384",
   "metadata": {},
   "source": [
    "#### User-Supplied Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bd7e0",
   "metadata": {},
   "source": [
    "Below you can specify datasets that you want to be tested in the benchmarking. You can either enter single files or multiple files that belong to a single dataset, but that dataset match at least one of the supported formats. **Read the following input rules after running the UI cell below this one.**\n",
    "\n",
    "\n",
    "1. Activate the checkbox if you desire to record your user-defined datasets. If it is not checked, none of your inputs will be recorded.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Input the full URI or absolute path of the data location (`<URI prefix>://bucket-name/path/to/file.extension` or `/path/to/file.extension`)\n",
    "    - Use globstrings (`<URI prefix>://bucket-name/path/to/files/*` or `path/to/files/*`) to specify datasets that are split up into multiple subfiles.\n",
    "    - If using a globstring, ensure that *only* files that belong to the dataset exist in that directory. The workflow will take all files in the directory before the `*` and attempt to gather them into a single dataset.\n",
    "    \n",
    "<br>\n",
    "\n",
    "3. If you have a dataset stored in multiple cloud storage locations that will be used in the benchmarking, you must input the full URI of that dataset for each of these locations. That is, you must define each location of the data separately.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. For single-file datasets > 5.4 GB (5 GiB) in size that may need to be transferred across clouds in the benchmarking, you must transfer these files to the desired locations *before* running the benchmarking.\n",
    "    - This is because `gsutil`, one of the tools the benchmark uses to copy user-defined files from the original cloud storage location to other benchmarking locations, can only handle single-file transfers between CSPs that are smaller than 5 GiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "userdata = ui.userdataWidgets(storage=storage)\n",
    "userdata.display()\n",
    "user_files = userdata.processInput()\n",
    "print(f'Your data inputs:\\n {user_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c5eb",
   "metadata": {},
   "source": [
    "#### Randomly-Generated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322692c1",
   "metadata": {},
   "source": [
    "Another option to supply data to the benchmarking is to create randomly-generated datasets. These sets can be as large as you want (as they are written in parallel), and provide a great option if you are new to the world of cloud-native data formats. There are currently two supported randomly-generated data formats: CSV and NetCDF4. Since NetCDF4 is a gridded data format, an option to specify the number of coordinate axes is also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46825cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "randgen = ui.randgenWidgets(resources=resources)\n",
    "randgen.display()\n",
    "randfiles = randgen.processInput()\n",
    "print(f'Your randomly-generated file options:\\n {randfiles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc612fa1",
   "metadata": {},
   "source": [
    "### TODO: Cloud-Native Format Conversion Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581d0ee",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d585b",
   "metadata": {},
   "source": [
    "## Step 2: Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f2c55",
   "metadata": {},
   "source": [
    "Executing the following cell will write all of your inputs to `inputs.json`, install miniconda3 and the \"cloud-data\" Python environment to all resources, and write randomly-generated files to all cloud storage locations (if any files were specified). If writing randomly-generated files, especially large ones, the execution of this cell may take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb642c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "While randomly-generated files are written in parallel by default, if you wish to speed up the execution of this cell, consider creating/choosing a resource with more powerful worker nodes.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff33e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Setting up workflow...')\n",
    "\n",
    "user_input = json.dumps({\"RESOURCES\" : resources,\n",
    "                         \"STORAGE\" : storage,\n",
    "                         \"USERFILES\" : user_files,\n",
    "                         \"RANDFILES\" : randfiles\n",
    "                        })\n",
    "\n",
    "with open('inputs.json', 'w') as outfile:\n",
    "    outfile.write(user_input)\n",
    "\n",
    "os.system(\"bash workflow_notebook_setup.sh\")\n",
    "\n",
    "print('Workflow setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36950650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will install miniconda3 to \"/home/jgreen/.miniconda3\"\n",
      "Installing Miniconda-latest on \"gcptestnew\"...\n",
      "Miniconda is already installed in \"/home/jgreen/.miniconda3\"!\n",
      "Finished installing Miniconda on \"gcptestnew\".\n",
      "\n",
      "Building \"cloud-data\" environment on \"gcptestnew\"...\n",
      "Environment already exists!\n",
      "Finished building \"cloud-data\" environment on \"gcptestnew\".\n",
      "\n",
      "Done installing Miniconda-latest and building `cloud-data` on all requested resources.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgreen/cloud-data-transfer-benchmarking/storage-keys/.cloud-data-benchmarks.json: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random files (this will take a while for large size requests)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgreen/.miniconda3/envs/cloud-data/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40110 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating NetCDF4...\n"
     ]
    }
   ],
   "source": [
    "os.system(\"bash workflow_notebook_setup.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ea9b",
   "metadata": {},
   "source": [
    "## Step 3: Run Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d260",
   "metadata": {},
   "source": [
    "### Convert File to Cloud-Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457bccf",
   "metadata": {},
   "source": [
    "Since one of the major goals of this benchmarking is testing legacy formats against cloud-native ones, we must convert your legacy-formatted data (CSV and NetCDF4) into their corresponding cloud-native formats. This cell will execute and time the conversion process, writing each new format in parallel. The conversion will be done using each cluster's full amount of resources, so be mindful of this feature when using clusters that are expensive to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4685034",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"convert-data.py\\\" \\\"conversions.csv\\\"\")\n",
    "df_conversions = pd.read_csv(os.getcwd() + '/results/csv-files/conversions.csv')\n",
    "df_conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702d935",
   "metadata": {},
   "source": [
    "### File Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433a5ac",
   "metadata": {},
   "source": [
    "The last computation-intensive test in the benchmarking is reading and timing files from cloud storage. This will give you an idea of what data transfer throughput you can expect when using cloud storage and different data formats in other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"read-data.py\\\" \\\"reads.csv\\\"\")\n",
    "df_reads = pd.read_csv(os.getcwd() + '/results/csv-files/reads.csv')\n",
    "df_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a169d8",
   "metadata": {},
   "source": [
    "## TODO: Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d69914",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
