{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06aad8f1",
   "metadata": {},
   "source": [
    "# Cloud Data Transfer Speeds Benchmarking Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551822",
   "metadata": {},
   "source": [
    "Add overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52c47",
   "metadata": {},
   "source": [
    "## Step 0: Load Required Setup Packages & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e427e0",
   "metadata": {},
   "source": [
    "Enter the following parameters to install packages to the correct conda instance and environment.\n",
    "\n",
    "\n",
    "`jupyter_conda_path : str`\n",
    "    - The path to miniconda that this Jupyter notebook is running from. Do not include a terminal `/` at the end of the path.\n",
    "    \n",
    "`jupyter_conda_env : str`\n",
    "    - The conda environment that this Jupyter notebook is running in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f93cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_conda_path = \"/home/jgreen/.miniconda3c\"\n",
    "jupyter_conda_env = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9c4fe",
   "metadata": {},
   "source": [
    "Installs required workflow setup packages and calls UI generation script. If one or more of the packages don't exist in the specified environment, they will install for you. Note that if installation is required, this cell will take a few minutes to complete execution.\n",
    "\n",
    "**NOTE: If you recieve an import error for `jupyter-ui-poll`, you will have to manually install the package in a user container terminal with the following commands:**\n",
    "```\n",
    "source <jupyter_conda_path>/etc/profile.d/conda.sh\n",
    "conda activate <jupyter_conda_env>\n",
    "pip install jupyter-ui-poll\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73185f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking conda environment for UI depedencies...\n",
      "All dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "print('Checking conda environment for UI depedencies...')\n",
    "os.system(\"bash \" + os.getcwd() + f\"/jupyter-helpers/install_ui_packages.sh {jupyter_conda_path} {jupyter_conda_env}\")\n",
    "print('All dependencies installed.')\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + '/jupyter-helpers')\n",
    "import ui_helpers as ui\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be35fd4",
   "metadata": {},
   "source": [
    "## Step 1: Define Workflow Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de7e39",
   "metadata": {},
   "source": [
    "Run the following cells to generate interactive widgets allowing you to enter all workflow inputs. **All inputs must be filled out to proceed with the benchmarking process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cb664",
   "metadata": {},
   "source": [
    "### Cloud Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7e51",
   "metadata": {},
   "source": [
    "#### Compute Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d055",
   "metadata": {},
   "source": [
    "Before defining anything else, the resources you intend to use with the benchmarking must be defined. Currently, only resources defined in the Parallel Works platform may be used. Also of note are options that will be passed to Dask: you must specify the number of cores and memory per worker node in the cluster. Without these values, Dask will not be able to submit jobs.\n",
    "\n",
    "In particular, these options are included so that you can form fair comparisons between different cloud service providers (CSPs). Generally, different CSPs won't have worker nodes with the exact same specs, and in order to achieve a fair comparison between two CSPs one cluster will have to limited to not exceed to the computational power of the another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376e50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For resources controlled from the Parallel Works platform, the <code>Resource name</code> box should be populated with the name found on the <b>RESOURCES</b>  tab.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21753c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Resource Name: '), Text(value=''))), HBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------\n",
      "If you wish to change information about cloud resources, run this cell again.\n",
      "\n",
      "Your resource inputs:\n",
      " [{'Name': 'gcptestnew', 'CSP': 'GCP', 'Dask': {'Scheduler': 'SLURM', 'Partition': 'compute', 'CPUs': 2, 'Memory': 16.0}}]\n"
     ]
    }
   ],
   "source": [
    "resource = ui.resourceWidgets()\n",
    "resource.display()\n",
    "resources = resource.processInput()\n",
    "print(f'Your resource inputs:\\n {resources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4fe8c",
   "metadata": {},
   "source": [
    "#### Object Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea7fb6",
   "metadata": {},
   "source": [
    "This set of inputs is where you enter the cloud object store Universal Resource Identifiers (URIs). Both public and private buckets are supported. For the latter, ensure that you have access credentials with *at least* read, write, list, and put (copy from local storage to cloud) permissions, as format conversions will need to be made during the benchmarking process.\n",
    "\n",
    "Be sure to double-check your inputs to ensure that the bucket names and credentials are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b144152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Storage URI: '), Text(value='', placeholder='gc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "If you wish to change information about cloud storage locations, run this cell again.\n",
      "\n",
      "Your storage inputs:\n",
      " [{'Path': 'gs://cloud-data-benchmarks', 'Type': 'Private', 'CSP': 'GCP', 'Credentials': {'token': './.cloud-data-benchmarks.json'}}]\n"
     ]
    }
   ],
   "source": [
    "store = ui.storageWidgets()\n",
    "store.display()\n",
    "storage = store.processInput()\n",
    "\n",
    "# Following line should be commented out if you don't want AWS credentials shown in plain text\n",
    "print(f'Your storage inputs:\\n {storage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711e2b",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323384",
   "metadata": {},
   "source": [
    "#### User-Supplied Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bd7e0",
   "metadata": {},
   "source": [
    "Below you can specify datasets that you want to be tested in the benchmarking. You can either enter single files or multiple files that belong to a single dataset, but that dataset match at least one of the supported formats. **Read the following input rules after running the UI cell below this one.**\n",
    "\n",
    "\n",
    "1. Activate the checkbox if you desire to record your user-defined datasets. If it is not checked, none of your inputs will be recorded.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. If speciying data in a bucket, input the path to the data within the bucket, *not* the full URI (i.e., use `path/to/file.extension` and not `<URI prefix>://<bucketname>/path/to/file.extension`)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Use absolute paths for data stored in the user container (or a filesystem mounted in the user container): `/path/to/data.extension`\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Use globstrings (`path/to/files/*`) to specify datasets that are split up into multiple subfiles.\n",
    "    - If using a globstring, ensure that *only* files that belong to the dataset exist in that directory. The workflow will take all files in the directory before the `*` and attempt to gather them into a single dataset.\n",
    "    - **Globstrings are NOT supported for NetCDF files**\n",
    "    \n",
    "<br>\n",
    "\n",
    "5. If you have a dataset stored in multiple cloud storage locations that will be used in the benchmarking, you must input the full URI of that dataset for each of these locations. That is, you must define each location of the data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0baaed80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Provide datasets to workflow?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Data Format'), Dropdown(options=('NetCDF4', 'CS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "If you wish to change information about your input data, run this cell again.\n",
      "\n",
      "Your data inputs:\n",
      " [{'Format': 'NetCDF4', 'SourcePath': 'gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.nc', 'DataVars': ['*'], 'Type': 'Private', 'CSP': 'GCP', 'Credentials': {'token': './.cloud-data-benchmarks.json'}}]\n"
     ]
    }
   ],
   "source": [
    "userdata = ui.userdataWidgets(storage=storage)\n",
    "userdata.display()\n",
    "user_files = userdata.processInput()\n",
    "print(f'Your data inputs:\\n {user_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c5eb",
   "metadata": {},
   "source": [
    "#### Randomly-Generated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322692c1",
   "metadata": {},
   "source": [
    "Another option to supply data to the benchmarking is to create randomly-generated datasets. These sets can be as large as you want (as they are written in parallel), and provide a great option if you are new to the world of cloud-native data formats. There are currently two supported randomly-generated data formats: CSV and NetCDF4. Since NetCDF4 is a gridded data format, an option to specify the number of coordinate axes is also included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf879954",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Randomly-generated NetCDF4 file sizes are limited by available disk space in the cluster you are generating the file with. Ensure that you have adequate disk space in your cluster, or the file will not fully generate.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46825cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HBox(children=(Label(value='Resource to write random files with: '), Dropdown(options=('gc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "If you wish to change the randomly generated file options, run this cell again.\n",
      "\n",
      "Your randomly-generated file options:\n",
      " [{'Format': 'CSV', 'Generate': False, 'SizeGB': 0.0}, {'Format': 'NetCDF4', 'Generate': True, 'SizeGB': 3.0, 'Data Variables': 1.0, 'Float Coords': 2.0}, {'Resource': 'gcptestnew'}]\n"
     ]
    }
   ],
   "source": [
    "randgen = ui.randgenWidgets(resources=resources)\n",
    "randgen.display()\n",
    "randfiles = randgen.processInput()\n",
    "print(f'Your randomly-generated file options:\\n {randfiles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765ab83",
   "metadata": {},
   "source": [
    "### Benchmark Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfffa69",
   "metadata": {},
   "source": [
    "#### Legacy to Cloud-Native Conversion Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfdd5e",
   "metadata": {},
   "source": [
    "There are a number of different options to choose from when legacy files (e.g., CSV, NetCDF) are converted to cloud-native (e.g., Parquet, Zarr). In fact, this input field is where you have the most control over the type of resutls you want to see. You may specify different compression algorithms, compression levels, and chunksizes to use when cloud-native files are written to cloud storage, which each have different effects on how fast files will be written and read.\n",
    "\n",
    "Note that, for large chunksizes, the workflow may fail. This is an intended consequence, as the workflow is designed to let you explore the effects of different options on your data. **In general, it is recommended that you keep chunksizes in the range of 50-150 megabytes**. Anything higher will result in either poor performance or out-of-memory errors. Similarly, some compression algorithms will result in poor performance.\n",
    "\n",
    "The widget below allows you to define different sets of options to convert files with. To make multiple selections in the compression algorithm and dataset fields, simply hold `SHIFT` and/or `CTRL` (or `COMMAND` on Mac). Using these option sets, the files defined in each set will be converted using the specified compression & chunksize information. The following rules apply:\n",
    "\n",
    "- An individual dataset will be converted `(# of compression algorithms in option set 1)*...*(# of compression algorithms in option set n)` times\n",
    "- The total number of conversions is equal to the sum of individual dataset conversions\n",
    "- Datasets will only be converted if they are selected in at least one option set.\n",
    "- **If you want to keep the internal chunking scheme of the NetCDF data, set chunksize to 0**\n",
    "\n",
    "Gridded data chunk dimensions are computed as n-dimensional cubes (i.e., a data variable described by 3 dimensions will have a 3-dimensional chunk of uniform dimension lengths, and so on for higher-order cases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3e88f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Data Format Type: '), Dropdown(options=('Gridde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "If you wish to change information about your conversion options, run this cell again.\n",
      "\n",
      "Your cloud-native data options:\n",
      " [{'Algorithms': ('lz4', 'lz4hc'), 'Level': 5, 'Chunksize': 0.0, 'Datasets': ('random_3.0GB_NetCDF4.nc', 'ETOPO1_Ice_g_gmt4.nc')}, {'Algorithms': ('lz4',), 'Level': 5, 'Chunksize': 150.0, 'Datasets': ('ETOPO1_Ice_g_gmt4.nc',)}]\n"
     ]
    }
   ],
   "source": [
    "convertOptions = ui.convertOptions(user_files, randfiles)\n",
    "convertOptions.display()\n",
    "convert_options = convertOptions.processInput()\n",
    "print(f'Your cloud-native data options:\\n {convert_options}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa84e5",
   "metadata": {},
   "source": [
    "#### Core Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303ad9f",
   "metadata": {},
   "source": [
    "Finally, there are a couple important options that apply to the core of the benchmarks. These are:\n",
    "\n",
    "1. `worker_step : int` - When performing file reads (option not yet implemented in conversions), the workflow will loop through a range of Dask workers (or number of parallel reads). The loop starts at the maximum defined worker amount, and reduces by `worker_step` until the lowest possible value of workers is reached.\n",
    "\n",
    "1. `tests : int` - The number of times each individual file will be read for measurement. Entering a number greater than 1 will take much longer to run, but results will reflect the volaility in network performance you can expect when using the data for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84346150",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_step = 5\n",
    "tests = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d585b",
   "metadata": {},
   "source": [
    "## Step 2: Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f2c55",
   "metadata": {},
   "source": [
    "Executing the following cell will write all of your inputs to `inputs.json`, install miniconda3 and the \"cloud-data\" Python environment to all resources, and write randomly-generated files to all cloud storage locations (if any files were specified). If writing randomly-generated files, especially large ones, the execution of this cell may take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb642c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "While randomly-generated files are written in parallel by default, if you wish to speed up the execution of this cell, consider creating/choosing a resource with more powerful worker nodes.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eff33e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up workflow...\n",
      "Will install miniconda3 to \"/home/jgreen/.miniconda3\"\n",
      "Installing Miniconda-latest on \"gcptestnew\"...\n",
      "Miniconda is already installed in \"/home/jgreen/.miniconda3\"!\n",
      "Finished installing Miniconda on \"gcptestnew\".\n",
      "\n",
      " \n",
      "Building \"cloud-data\" environment on \"gcptestnew\"...\n",
      "Environment already exists!\n",
      "Finished building \"cloud-data\" environment on \"gcptestnew\".\n",
      "\n",
      "Done installing Miniconda-latest and building `cloud-data` on all requested resources.\n",
      "\n",
      "\n",
      "Generating random files (this will take a while for large size requests)...\n",
      "Generating NetCDF4...\n",
      "Writing data variable \"random1\"...\n",
      "Data variable \"random1\" written to \"./random_3.0GB_NetCDF4.nc\"\n",
      "Uploading \"./random_3.0GB_NetCDF4.nc\" to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/randfiles/random_3.0GB_NetCDF4.nc\"...\n",
      "File written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/randfiles/random_3.0GB_NetCDF4.nc\"\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 22:24:16,812 - distributed.deploy.adaptive_core - INFO - Adaptive stop\n",
      "2023-08-10 22:24:16,954 - distributed.deploy.adaptive_core - INFO - Adaptive stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow setup complete.\n"
     ]
    }
   ],
   "source": [
    "print('Setting up workflow...')\n",
    "\n",
    "user_input = json.dumps({\"RESOURCES\" : resources,\n",
    "                         \"STORAGE\" : storage,\n",
    "                         \"USERFILES\" : user_files,\n",
    "                         \"RANDFILES\" : randfiles,\n",
    "                         \"CONVERTOPTS\" : convert_options,\n",
    "                         \"GLOBALOPTS\" : {'worker_step' : worker_step, 'tests' : tests}\n",
    "                        })\n",
    "\n",
    "with open('inputs.json', 'w') as outfile:\n",
    "    outfile.write(user_input)\n",
    "\n",
    "os.system(\"bash workflow_notebook_setup.sh\")\n",
    "\n",
    "print('Workflow setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ea9b",
   "metadata": {},
   "source": [
    "## Step 3: Run Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d260",
   "metadata": {},
   "source": [
    "### Convert File to Cloud-Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad150c9",
   "metadata": {},
   "source": [
    "Since one of the major goals of this benchmarking is testing legacy formats against cloud-native ones, we must convert your legacy-formatted data (CSV and NetCDF4) into their corresponding cloud-native formats. This cell will execute and time the conversion process, writing each new format in parallel. The conversion will be done using each cluster's full amount of resources, so be mindful of this feature when using clusters that are expensive to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4685034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for worker nodes to start up...\n",
      "Workers active.\n",
      "\n",
      "\n",
      "Converting files in \"gs://cloud-data-benchmarks\" with \"gcptestnew\"...\n",
      "\n",
      "Converting data variables Z1 from ETOPO1_Ice_g_gmt4.nc with 0.0MB chunks & level 5 lz4 compression to Zarr...\n",
      "Written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr\"\n",
      "Converting data variables Z1 from ETOPO1_Ice_g_gmt4.nc with 0.0MB chunks & level 5 lz4hc compression to Zarr...\n",
      "Written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr\"\n",
      "Converting data variables Z1 from ETOPO1_Ice_g_gmt4.nc with 150.0MB chunks & level 5 lz4 compression to Zarr...\n",
      "Written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr\"\n",
      "Converting data variables random1 from random_3.0GB_NetCDF4.nc with 0.0MB chunks & level 5 lz4 compression to Zarr...\n",
      "Written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr\"\n",
      "Converting data variables random1 from random_3.0GB_NetCDF4.nc with 0.0MB chunks & level 5 lz4hc compression to Zarr...\n",
      "Written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr\"\n",
      "Done converting files in \"gs://cloud-data-benchmarks\".\n",
      "Shutting down worker nodes...\n",
      "Workers shut down. (this may take a while to register in the platform UI)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource</th>\n",
       "      <th>resource_csp</th>\n",
       "      <th>bucket</th>\n",
       "      <th>bucket_csp</th>\n",
       "      <th>conversionType</th>\n",
       "      <th>original_dataset_name</th>\n",
       "      <th>data_variables</th>\n",
       "      <th>conversion_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>9.302547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>6.441293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>5.708016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>random_3.0GB_NetCDF4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>18.860593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>random_3.0GB_NetCDF4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>16.569385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     resource resource_csp                      bucket bucket_csp  \\\n",
       "0  gcptestnew          GCP  gs://cloud-data-benchmarks        GCP   \n",
       "1  gcptestnew          GCP  gs://cloud-data-benchmarks        GCP   \n",
       "2  gcptestnew          GCP  gs://cloud-data-benchmarks        GCP   \n",
       "3  gcptestnew          GCP  gs://cloud-data-benchmarks        GCP   \n",
       "4  gcptestnew          GCP  gs://cloud-data-benchmarks        GCP   \n",
       "\n",
       "   conversionType    original_dataset_name data_variables  conversion_time  \n",
       "0  NetCDF-to-Zarr     ETOPO1_Ice_g_gmt4.nc              *         9.302547  \n",
       "1  NetCDF-to-Zarr     ETOPO1_Ice_g_gmt4.nc              *         6.441293  \n",
       "2  NetCDF-to-Zarr     ETOPO1_Ice_g_gmt4.nc              *         5.708016  \n",
       "3  NetCDF-to-Zarr  random_3.0GB_NetCDF4.nc              *        18.860593  \n",
       "4  NetCDF-to-Zarr  random_3.0GB_NetCDF4.nc              *        16.569385  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"convert-data.py\\\" \\\"conversions.csv\\\"\")\n",
    "df_conversions = pd.read_csv(os.getcwd() + '/results/csv-files/conversions.csv')\n",
    "df_conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702d935",
   "metadata": {},
   "source": [
    "### File Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffc4ec",
   "metadata": {},
   "source": [
    "The last computation-intensive test in the benchmarking is reading and timing files from cloud storage. This will give you an idea of what data transfer throughput you can expect when using cloud storage and different data formats in other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"read-data.py\\\" \\\"reads.csv\\\"\")\n",
    "df_reads = pd.read_csv(os.getcwd() + '/results/csv-files/reads.csv')\n",
    "df_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a169d8",
   "metadata": {},
   "source": [
    "## TODO: Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d69914",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fb9e7",
   "metadata": {},
   "source": [
    "## Step 5: Remove Benchmarking Files from Cloud Resources (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a9b9a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/.zattrs#1691706641280223...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/.zgroup#1691706640643778...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/.zmetadata#1691706642598058...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/.zattrs#1691706642247161...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/.zarray#1691706641982713...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/0.0#1691706645918238...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/0.1#1691706646572791...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/1.0#1691706648582987...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/1.1#1691706649263686...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/2.0#1691706649169530...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/5.0#1691706649185364...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/2.1#1691706648562264...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/3.0#1691706648531062...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/0.1#1691706654665834...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/3.1#1691706649162563...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/4.0#1691706648292352...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/5.1#1691706648428236...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/1.0#1691706655190297...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/6.0#1691706648216599...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/6.1#1691706648228392...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/.zgroup#1691706649942132...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4_5.zarr/Z1/4.1#1691706648534367...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/5.1#1691706654598766...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/.zmetadata#1691706651801382...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/.zattrs#1691706650456198...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/.zarray#1691706651190679...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/.zattrs#1691706651481542...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/3.0#1691706655615518...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/1.1#1691706655256162...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/3.1#1691706654818118...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/4.0#1691706655018516...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/4.1#1691706655581481...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/5.0#1691706654387980...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/0.0#1691706654188441...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/6.0#1691706654519256...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/6.1#1691706654166549...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/.zattrs#1691706656760703...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/2.0#1691706654305040...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_0.0MB_lz4hc_5.zarr/Z1/2.1#1691706655698405...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/.zgroup#1691706656288069...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/.zmetadata#1691706658015471...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/.zarray#1691706657464214...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/.zattrs#1691706657705469...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/0.0#1691706660797753...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/0.1#1691706660668124...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/0.2#1691706660684556...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/0.3#1691706660587618...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/0.4#1691706660674212...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/1.0#1691706660980156...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/1.1#1691706661053869...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/1.2#1691706661406171...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/1.3#1691706660996189...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/1.4#1691706661397324...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/2.0#1691706660343208...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/2.1#1691706660384405...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/2.2#1691706661046176...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/2.3#1691706660851496...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_150.0MB_lz4_5.zarr/Z1/2.4#1691706660732546...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/.zattrs#1691706663197347...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/.zgroup#1691706662205584...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/.zmetadata#1691706668106063...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/f1/.zattrs#1691706666482588...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/f1/0#1691706666615281...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/f2/.zarray#1691706667246610...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/f2/.zattrs#1691706667487541...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/f1/.zarray#1691706666210233...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/f2/0#1691706667627746...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.0.0#1691706672363517...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/.zattrs#1691706665598830...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.1.0#1691706670688374...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.1.2#1691706678233492...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.1.1#1691706674780962...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.0.2#1691706674118255...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/.zarray#1691706665346955...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.0.1#1691706672948048...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.2.0#1691706678161498...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.2.1#1691706677914612...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/0.2.2#1691706673519636...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.0.0#1691706675296276...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.0.1#1691706672101273...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.0.2#1691706676748683...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.1.0#1691706674573693...\n",
      "/ [43/141 objects]  30% Done                                                    \r",
      "/ [44/141 objects]  31% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.1.1#1691706675331569...\n",
      "/ [45/141 objects]  31% Done                                                    \r",
      "/ [46/141 objects]  32% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.1.2#1691706679926052...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.2.1#1691706673665963...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.2.0#1691706673949494...\n",
      "/ [47/141 objects]  33% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/1.2.2#1691706675632486...\n",
      "/ [48/141 objects]  34% Done                                                    \r",
      "/ [49/141 objects]  34% Done                                                    \r",
      "/ [50/141 objects]  35% Done                                                    \r",
      "/ [51/141 objects]  36% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.0.0#1691706670943280...\n",
      "/ [52/141 objects]  36% Done                                                    \r",
      "/ [53/141 objects]  37% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.0.1#1691706674338701...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.0.2#1691706674924598...\n",
      "/ [54/141 objects]  38% Done                                                    \r",
      "/ [55/141 objects]  39% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.1.0#1691706670323591...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.1.1#1691706676663471...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.1.2#1691706680496747...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.2.0#1691706673734222...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.2.1#1691706678608926...\n",
      "/ [56/141 objects]  39% Done                                                    \r",
      "/ [57/141 objects]  40% Done                                                    \r",
      "/ [58/141 objects]  41% Done                                                    \r",
      "/ [59/141 objects]  41% Done                                                    \r",
      "/ [60/141 objects]  42% Done                                                    \r",
      "/ [61/141 objects]  43% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/random1/2.2.2#1691706673629593...\n",
      "/ [62/141 objects]  43% Done                                                    \r",
      "/ [63/141 objects]  44% Done                                                    \r",
      "/ [64/141 objects]  45% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/.zattrs#1691706682021485...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/t1/.zarray#1691706663859429...\n",
      "/ [65/141 objects]  46% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/t1/0#1691706664249133...\n",
      "/ [66/141 objects]  46% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4_5.zarr/t1/.zattrs#1691706664105153...\n",
      "/ [67/141 objects]  47% Done                                                    \r",
      "/ [68/141 objects]  48% Done                                                    \r",
      "/ [69/141 objects]  48% Done                                                    \r",
      "/ [70/141 objects]  49% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/f1/.zarray#1691706684755756...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/.zgroup#1691706681053775...\n",
      "/ [71/141 objects]  50% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/f1/.zattrs#1691706685009588...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/.zmetadata#1691706686736284...\n",
      "/ [72/141 objects]  51% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/f1/0#1691706685167311...\n",
      "/ [73/141 objects]  51% Done                                                    \r",
      "/ [74/141 objects]  52% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/f2/0#1691706686224519...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/f2/.zarray#1691706685799518...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/.zarray#1691706683861887...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/.zattrs#1691706684106951...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.0.0#1691706689940342...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/f2/.zattrs#1691706686087290...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.0.1#1691706691277638...\n",
      "/ [75/141 objects]  53% Done                                                    \r",
      "/ [76/141 objects]  53% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.0.2#1691706692284303...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.1.0#1691706689316373...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.1.1#1691706689763339...\n",
      "/ [77/141 objects]  54% Done                                                    \r",
      "/ [78/141 objects]  55% Done                                                    \r",
      "/ [79/141 objects]  56% Done                                                    \r",
      "/ [80/141 objects]  56% Done                                                    \r",
      "/ [81/141 objects]  57% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.1.2#1691706695002025...\n",
      "/ [82/141 objects]  58% Done                                                    \r",
      "/ [83/141 objects]  58% Done                                                    \r",
      "/ [84/141 objects]  59% Done                                                    \r",
      "/ [85/141 objects]  60% Done                                                    \r",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.2.0#1691706691901888...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.2.2#1691706696061614...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.0.2#1691706697055687...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.0.0#1691706692436664...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/0.2.1#1691706693075367...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.0.1#1691706692268810...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.1.1#1691706694025850...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.1.2#1691706694894079...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.1.0#1691706689522153...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.2.0#1691706691568508...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.2.1#1691706693342230...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/1.2.2#1691706692634600...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.0.0#1691706693182209...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.0.1#1691706693914858...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.0.2#1691706692768391...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.2.0#1691706690301182...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.1.0#1691706692702161...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.2.1#1691706693619009...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.1.1#1691706695136954...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.1.2#1691706692420161...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/random1/2.2.2#1691706690340882...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/t1/.zarray#1691706682732829...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/t1/.zattrs#1691706682991323...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/random_3.0GB_NetCDF4.nc_0.0MB_lz4hc_5.zarr/t1/0#1691706683177163...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/randfiles/random_3.0GB_NetCDF4.nc#1691706256719950...\n",
      "/ [141/141 objects] 100% Done                                                   \n",
      "Operation completed over 141 objects.                                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"bash postprocessing/remove-benchmark-files.sh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
