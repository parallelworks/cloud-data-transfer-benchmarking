{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06aad8f1",
   "metadata": {},
   "source": [
    "# Cloud Data Transfer Speeds Benchmarking Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551822",
   "metadata": {},
   "source": [
    "Add overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52c47",
   "metadata": {},
   "source": [
    "## Step 0: Load Required Setup Packages & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e427e0",
   "metadata": {},
   "source": [
    "Enter the following parameters to install packages to the correct conda instance and environment.\n",
    "\n",
    "\n",
    "`jupyter_conda_path : str`\n",
    "    - The path to miniconda that this Jupyter notebook is running from. Do not include a terminal `/` at the end of the path.\n",
    "    \n",
    "`jupyter_conda_env : str`\n",
    "    - The conda environment that this Jupyter notebook is running in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f93cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PW_USER is an alternative\n",
    "my_user_name = os.environ['USER']\n",
    "\n",
    "# Default location for Conda from Jupyter workflow\n",
    "jupyter_conda_path = f\"/home/{my_user_name}/.miniconda3c\"\n",
    "jupyter_conda_env = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9c4fe",
   "metadata": {},
   "source": [
    "Installs required workflow setup packages and calls UI generation script. If one or more of the packages don't exist in the specified environment, they will install for you. Note that if installation is required, this cell will take a few minutes to complete execution.\n",
    "\n",
    "**NOTE: If you recieve an import error for `jupyter-ui-poll`, you will have to manually install the package in a user container terminal with the following commands:**\n",
    "```\n",
    "source <jupyter_conda_path>/etc/profile.d/conda.sh\n",
    "conda activate <jupyter_conda_env>\n",
    "pip install jupyter-ui-poll\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73185f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking conda environment for UI depedencies...\n",
      "All dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "print('Checking conda environment for UI depedencies...')\n",
    "os.system(\"bash \" + os.getcwd() + f\"/jupyter-helpers/install_ui_packages.sh {jupyter_conda_path} {jupyter_conda_env}\")\n",
    "print('All dependencies installed.')\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + '/jupyter-helpers')\n",
    "import ui_helpers as ui\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be35fd4",
   "metadata": {},
   "source": [
    "## Step 1: Define Workflow Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de7e39",
   "metadata": {},
   "source": [
    "Run the following cells to provide your workflow inputs. Simple inputs are handled by normal Python variables, while interactive widgets are generated for the more complicated options.  **All inputs must be filled out to proceed with the benchmarking process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cb664",
   "metadata": {},
   "source": [
    "### Cloud Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7e51",
   "metadata": {},
   "source": [
    "#### Compute Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d055",
   "metadata": {},
   "source": [
    "Before defining anything else, the resources you intend to use with the benchmarking must be defined. Currently, only resources defined in the Parallel Works platform may be used. Also of note are options that will be passed to Dask: you must specify the number of cores and memory per worker node in the cluster. Without these values, Dask will not be able to submit jobs.\n",
    "\n",
    "In particular, these options are included so that you can form fair comparisons between different cloud service providers (CSPs). Generally, different CSPs won't have worker nodes with the exact same specs, and in order to achieve a fair comparison between two CSPs one cluster must be limited such that it does not exceed the computational power of the other.\n",
    "\n",
    "Before submitting your options, read and adhere to the following guidelines:\n",
    "\n",
    "- **When entering the number of CPUs in the following widget, you must know if the worker nodes of your clusters have hyperthreading enabled.** The instance type of a partition in the resource definition will display a number of vCPUs: this number represents the number of CPUs with hyperthreading enabled. **If hyperthreading is disabled, enter the number of vCPUs divided by 2 into this workflow.** If you do not know, assume that hyperthreading is disabled.\n",
    "\n",
    "</br>\n",
    "\n",
    "- Supply a path to an existing miniconda installation or the desired installation path. Leave the default option of `~` to install to the home directory of the cluster. **Miniconda must be installed in a directory accessible to the head node and all worker nodes. If you are unsure if your current/desired installation directory is a shared directory, use the default option.**\n",
    "\n",
    "</br>\n",
    "\n",
    "- For `SSH Hostname`, go to the `COMPUTE` tab, find your resource in the `Computing Resources` section, and click on the `<user>@<IP>` shown in the upper right-hand corner of the resource's dashboard. Paste this into the `SSH Hostname` field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376e50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    " The resource name is an arbitrary field used only to help you identify when the benchmark is running on that particular cluster; you may set this to whatever you want. Additional information such as memory and cores can be found in the <code>Instance Type</code> field in the resource definition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21753c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Resource Name: '), Text(value='cluster1'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------\n",
      "If you wish to change information about cloud resources, run this cell again.\n",
      "\n",
      "Your resource inputs:\n",
      " [{'Name': 'gcptestnew', 'SSH': 'jgreen@34.31.86.225', 'MinicondaDir': '~', 'CSP': 'GCP', 'Dask': {'Scheduler': 'SLURM', 'Partition': 'compute', 'CPUs': 2, 'Memory': 16.0}}]\n"
     ]
    }
   ],
   "source": [
    "resource = ui.resourceWidgets()\n",
    "resource.display()\n",
    "resources = resource.processInput()\n",
    "print(f'Your resource inputs:\\n {resources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4fe8c",
   "metadata": {},
   "source": [
    "#### Object Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea7fb6",
   "metadata": {},
   "source": [
    "This set of inputs is where you enter the cloud object store Universal Resource Identifiers (URIs). Both public and private buckets are supported. For the latter, ensure that you have access credentials with *at least* read, write, list, and put (copy from local storage to cloud) permissions, as format conversions will need to be made during the benchmarking process.\n",
    "\n",
    "Be sure to double-check your inputs to ensure that the bucket names and credentials are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b144152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Storage URI: '), Text(value='', placeholder='gc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "If you wish to change information about cloud storage locations, run this cell again.\n",
      "\n",
      "Your storage inputs:\n",
      " [{'Path': 'gs://cloud-data-benchmarks', 'Type': 'Private', 'CSP': 'GCP', 'Credentials': {'token': './.cloud-data-benchmarks.json'}}]\n"
     ]
    }
   ],
   "source": [
    "store = ui.storageWidgets()\n",
    "store.display()\n",
    "storage = store.processInput()\n",
    "\n",
    "# Following line should be commented out if you don't want AWS S3 credentials shown in plain text\n",
    "print(f'Your storage inputs:\\n {storage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711e2b",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323384",
   "metadata": {},
   "source": [
    "#### User-Supplied Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bd7e0",
   "metadata": {},
   "source": [
    "Below you can specify datasets that you want to be tested in the benchmarking. You can either enter single files or multiple files that belong to a single dataset, provided that the dataset matches one of the supported formats. **Read the following input rules after running the UI cell below this one.**\n",
    "\n",
    "\n",
    "1. Activate the checkbox if you desire to record your user-defined datasets. **If it is not checked, none of your inputs will be recorded.**\n",
    "\n",
    "<br>\n",
    "\n",
    "2. If speciying data in a bucket, input the path to the data within the bucket, *not* the full URI (i.e., use `path/to/file.extension` and not `<URI prefix>://<bucketname>/path/to/file.extension`)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Use absolute paths for data stored in the user container (or a filesystem mounted in the user container): `/path/to/data.extension`\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Use globstrings (`path/to/files/*`) to specify datasets that are split up into multiple subfiles.\n",
    "    - If using a globstring, ensure that *only* files that belong to the dataset exist in that directory. The workflow will take all files in the directory before the `*` and attempt to gather them into a single dataset.\n",
    "    - **Globstrings are NOT supported for NetCDF files**\n",
    "    \n",
    "<br>\n",
    "\n",
    "5. If you have a dataset stored in multiple cloud storage locations that will be used in the benchmarking, you must define an input for each of the locations. That is, you must define each location of the data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0baaed80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Provide datasets to workflow?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Data Format'), Dropdown(options=('NetCDF4', 'CS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "If you wish to change information about your input data, run this cell again.\n",
      "\n",
      "Your data inputs:\n",
      " [{'Format': 'NetCDF4', 'SourcePath': 'gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.nc', 'DataVars': ['*'], 'Type': 'Private', 'CSP': 'GCP', 'Credentials': {'token': './.cloud-data-benchmarks.json'}}]\n"
     ]
    }
   ],
   "source": [
    "userdata = ui.userdataWidgets(storage=storage)\n",
    "userdata.display()\n",
    "user_files = userdata.processInput()\n",
    "print(f'Your data inputs:\\n {user_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c5eb",
   "metadata": {},
   "source": [
    "#### Randomly-Generated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322692c1",
   "metadata": {},
   "source": [
    "Another option to supply data to the benchmarking is to create randomly-generated datasets. CSV datasets can be as large as you want and provide a great option if you are new to the world of cloud-native data formats. There are currently two supported randomly-generated data formats: CSV and NetCDF4. Since NetCDF4 is a gridded data format, options to specify the number of coordinate axes and data variables are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf879954",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Randomly-generated NetCDF4 file sizes are limited by available disk space in the cluster you are generating the file with. Ensure that you have adequate disk space in your cluster, or the file will not fully generate.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46825cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HBox(children=(Label(value='Resource to write random files with: '), Dropdown(options=('gc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "If you wish to change the randomly generated file options, run this cell again.\n",
      "\n",
      "Your randomly-generated file options:\n",
      " [{'Format': 'CSV', 'Generate': False, 'SizeGB': 0.0}, {'Format': 'NetCDF4', 'Generate': False, 'SizeGB': 0.0, 'Data Variables': 1.0, 'Float Coords': 2.0}, {'Resource': 'gcptestnew'}]\n"
     ]
    }
   ],
   "source": [
    "randgen = ui.randgenWidgets(resources=resources)\n",
    "randgen.display()\n",
    "randfiles = randgen.processInput()\n",
    "print(f'Your randomly-generated file options:\\n {randfiles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765ab83",
   "metadata": {},
   "source": [
    "### Benchmark Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfffa69",
   "metadata": {},
   "source": [
    "#### Legacy to Cloud-Native Conversion Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfdd5e",
   "metadata": {},
   "source": [
    "There are a number of different options to choose from when legacy files (e.g., CSV, NetCDF) are converted to cloud-native (e.g., Parquet, Zarr). In fact, this input field is where you have the most control over the type of resutls you want to see. You may specify different compression algorithms, compression levels, and chunksizes to use when cloud-native files are written to cloud storage, which each have different effects on how fast files will be written and read.\n",
    "\n",
    "Note that for large chunksizes the workflow may fail. This is an intended consequence, as the workflow is designed to let you explore the effects of different options on your data. **In general, it is recommended that you keep chunksizes in the range of 50-150 megabytes**: anything higher will result in either poor performance or out-of-memory errors. Similarly, some compression algorithms will result in poor performance depending on the data type.\n",
    "\n",
    "The widget below allows you to define different sets of options to convert files with. To make multiple selections in the compression algorithm and dataset fields, simply hold `SHIFT` and/or `CTRL` (or `COMMAND` on Mac). Using these option sets, the files defined in each set will be converted using the specified compression & chunksize information. The following rules apply:\n",
    "\n",
    "- An individual dataset will be converted `(# of compression algorithms in option set 1)*...*(# of compression algorithms in option set n)` times\n",
    "- The total number of conversions is equal to the sum of individual dataset conversions\n",
    "- **Datasets will only be converted if they are *highlighted* in at least one option set. That is, you must click on a dataset from the datasets list if you want the conversion options to apply to it.**\n",
    "- **If you want to keep the internal chunking scheme of a NetCDF4 dataset, set chunksize to 0**\n",
    "\n",
    "Gridded data chunk dimensions are computed as n-dimensional cubes (i.e., a data variable described by 3 dimensions will have a 3-dimensional chunk of uniform dimension lengths, and so on for higher-order cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3e88f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add field', style=ButtonStyle()), Button(description='Remove field', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Label(value='Data Format Type: '), Dropdown(options=('Gridde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "If you wish to change information about your conversion options, run this cell again.\n",
      "\n",
      "Your cloud-native data options:\n",
      " [{'Algorithms': ('lz4',), 'Level': 5, 'Chunksize': 0.0, 'Datasets': ('ETOPO1_Ice_g_gmt4.nc',)}]\n"
     ]
    }
   ],
   "source": [
    "convertOptions = ui.convertOptions(user_files, randfiles)\n",
    "convertOptions.display()\n",
    "convert_options = convertOptions.processInput()\n",
    "print(f'Your cloud-native data options:\\n {convert_options}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa84e5",
   "metadata": {},
   "source": [
    "#### Core Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303ad9f",
   "metadata": {},
   "source": [
    "Finally, there are three very important options that apply to the core of the benchmarks. **All of the following parameters must be nonnegative:**\n",
    "\n",
    "- `workers_per_node : int` - The number of Dask workers (parallel operations) each worker node of a cluster should spawn while carrying out a computation. Writing and reading from cloud storage is almost always bandwidth limited; in general, the more individual streams of data flowing into your worker nodes, the slower operations using data stored in the cloud will be. \n",
    "\n",
    "    This parameter is included to give you control of this behavior. Larger instance types often include increased network bandwidth compared to smaller ones (varies by CSP), and are able to support more parallel operations than the latter before hitting the instance's bandwidth limit.\n",
    "\n",
    "</br>\n",
    "\n",
    "- `worker_step : int` - When performing file reads (option not yet implemented in conversions), the workflow will loop through a range of Dask workers (or number of parallel reads). The loop starts at the maximum defined worker amount, and reduces by `worker_step` until the lowest possible value of workers is reached. This value must be a multiple of `workers_per_node`\n",
    "\n",
    "</br>\n",
    "\n",
    "- `tests : int` - The number of times each individual file will be read for measurement. Entering a number greater than 1 will take much longer to run, but results will reflect the volaility in network performance you can expect when using the data for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84346150",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_per_node = 2\n",
    "worker_step = 3 * workers_per_node\n",
    "tests = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d585b",
   "metadata": {},
   "source": [
    "## Step 2: Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f2c55",
   "metadata": {},
   "source": [
    "Executing the following cell will write all of your inputs to `inputs.json`, install miniconda3 and the \"cloud-data\" Python environment to all resources, and write randomly-generated files to all cloud storage locations (if any files were specified). If writing randomly-generated files, especially large ones, the execution of this cell may take a while.\n",
    "\n",
    "During this setup, clusters will be limited such that operations performed with each have identical Dask workers per node, cores per Dask worker, and memory per Dask worker. This may result in worker nodes of a particular cluster not using a large portion of their resources: this is indended behavior designed to allow for fair comparisons between different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb642c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "While randomly-generated files are written in parallel by default, if you wish to speed up the execution of this cell, consider creating/choosing a resource with more powerful worker nodes.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eff33e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up workflow...\n",
      "Will install miniconda3 to \"/home/jgreen/.miniconda3\"\n",
      "Installing Miniconda-23.5.2 on \"gcptestnew\"...\n",
      "Miniconda is already installed in \"/home/jgreen/.miniconda3\"!\n",
      "Finished installing Miniconda on \"gcptestnew\".\n",
      "\n",
      "Building \"cloud-data\" environment on \"gcptestnew\"...\n",
      "Environment already exists!\n",
      "Finished building \"cloud-data\" environment on \"gcptestnew\".\n",
      "\n",
      "Done installing Miniconda-23.5.2 and building `cloud-data` on all requested resources.\n",
      "\n",
      "\n",
      "Workflow setup complete.\n"
     ]
    }
   ],
   "source": [
    "print('Setting up workflow...')\n",
    "\n",
    "user_input = json.dumps({\"RESOURCES\" : resources,\n",
    "                         \"STORAGE\" : storage,\n",
    "                         \"USERFILES\" : user_files,\n",
    "                         \"RANDFILES\" : randfiles,\n",
    "                         \"CONVERTOPTS\" : convert_options,\n",
    "                         \"GLOBALOPTS\" : {'worker_step' : worker_step,\n",
    "                                         'tests' : tests,\n",
    "                                         'Dask': {'Workers': workers_per_node},\n",
    "                                         'local_conda_sh' : jupyter_conda_path}\n",
    "                        })\n",
    "\n",
    "minimum_cpus = min([v['Dask']['CPUs'] for v in resources])\n",
    "if workers_per_node > minimum_cpus:\n",
    "    print(f'Choose a value of \\'workers_per_node\\' that does not exceed {minimum_cpus}')\n",
    "\n",
    "else:\n",
    "    with open('ipynb_inputs.json', 'w') as outfile:\n",
    "        outfile.write(user_input)\n",
    "\n",
    "    os.system(\"bash workflow_setup.sh \\\"ipynb_inputs.json\\\"\")\n",
    "\n",
    "    print('Workflow setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ea9b",
   "metadata": {},
   "source": [
    "## Step 3: Run Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d260",
   "metadata": {},
   "source": [
    "### Convert File to Cloud-Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad150c9",
   "metadata": {},
   "source": [
    "Since one of the major goals of this benchmarking is testing legacy formats against cloud-native ones, we must convert your legacy-formatted data (CSV and NetCDF4) into their corresponding cloud-native formats. This cell will execute and time the conversion process, writing each new format in parallel. The conversion will be done using each cluster's full amount of resources, so be mindful of this feature when using clusters that are expensive to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4685034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for worker nodes to start up...\n",
      "Workers active.\n",
      "\n",
      "\n",
      "Converting files in \"gs://cloud-data-benchmarks\" with \"gcptestnew\"...\n",
      "\n",
      "Converting data variables \"Z1\" from \"ETOPO1_Ice_g_gmt4.nc\" with 134.203968MB chunks & level 5 lz4 compression to Zarr...\n",
      "Written to \"gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr\"\n",
      "Done converting files in \"gs://cloud-data-benchmarks\".\n",
      "Shutting down worker nodes...\n",
      "Workers shut down. (this may take a while to register in the platform UI)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncores</th>\n",
       "      <th>resource</th>\n",
       "      <th>resource_csp</th>\n",
       "      <th>bucket</th>\n",
       "      <th>bucket_csp</th>\n",
       "      <th>conversionType</th>\n",
       "      <th>orig_dataset_name</th>\n",
       "      <th>data_vars</th>\n",
       "      <th>orig_mem_size_bytes</th>\n",
       "      <th>orig_cloud_size_bytes</th>\n",
       "      <th>compr_alg</th>\n",
       "      <th>compr_lvl</th>\n",
       "      <th>chunksize_MB</th>\n",
       "      <th>conversion_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>362236855</td>\n",
       "      <td>lz4</td>\n",
       "      <td>5</td>\n",
       "      <td>134.203968</td>\n",
       "      <td>8.324833631515503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ncores</td>\n",
       "      <td>resource</td>\n",
       "      <td>resource_csp</td>\n",
       "      <td>bucket</td>\n",
       "      <td>bucket_csp</td>\n",
       "      <td>conversionType</td>\n",
       "      <td>orig_dataset_name</td>\n",
       "      <td>data_vars</td>\n",
       "      <td>orig_mem_size_bytes</td>\n",
       "      <td>orig_cloud_size_bytes</td>\n",
       "      <td>compr_alg</td>\n",
       "      <td>compr_lvl</td>\n",
       "      <td>chunksize_MB</td>\n",
       "      <td>conversion_time_seconds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF-to-Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>*</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>362236855</td>\n",
       "      <td>lz4</td>\n",
       "      <td>5</td>\n",
       "      <td>134.203968</td>\n",
       "      <td>12.298370361328125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ncores    resource  resource_csp                      bucket  bucket_csp  \\\n",
       "0      20  gcptestnew           GCP  gs://cloud-data-benchmarks         GCP   \n",
       "1  ncores    resource  resource_csp                      bucket  bucket_csp   \n",
       "2      20  gcptestnew           GCP  gs://cloud-data-benchmarks         GCP   \n",
       "\n",
       "   conversionType     orig_dataset_name  data_vars  orig_mem_size_bytes  \\\n",
       "0  NetCDF-to-Zarr  ETOPO1_Ice_g_gmt4.nc          *           1866499208   \n",
       "1  conversionType     orig_dataset_name  data_vars  orig_mem_size_bytes   \n",
       "2  NetCDF-to-Zarr  ETOPO1_Ice_g_gmt4.nc          *           1866499208   \n",
       "\n",
       "   orig_cloud_size_bytes  compr_alg  compr_lvl  chunksize_MB  \\\n",
       "0              362236855        lz4          5    134.203968   \n",
       "1  orig_cloud_size_bytes  compr_alg  compr_lvl  chunksize_MB   \n",
       "2              362236855        lz4          5    134.203968   \n",
       "\n",
       "   conversion_time_seconds  \n",
       "0        8.324833631515503  \n",
       "1  conversion_time_seconds  \n",
       "2       12.298370361328125  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"ipynb_inputs.json\\\" \\\"convert-data.py\\\" \\\"conversions.csv\\\"\")\n",
    "df_conversions = pd.read_csv(os.getcwd() + '/results/csv-files/conversions.csv')\n",
    "df_conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702d935",
   "metadata": {},
   "source": [
    "### File Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffc4ec",
   "metadata": {},
   "source": [
    "The last computation-intensive test in the benchmarking is reading and timing files from cloud storage. This will give you an idea of what data transfer throughput you can expect when using cloud storage and different data formats in other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "210bd34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535.1893860129277"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1866499208 / 1e6 / 3.487549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4944dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reading files in \"gs://cloud-data-benchmarks\" with \"gcptestnew\"...\n",
      "\n",
      "Reading the variable \"Z1\" from \"ETOPO1_Ice_g_gmt4.nc\"...\n",
      "Active Workers: 20\n",
      "Active Workers: 14\n",
      "Active Workers: 8\n",
      "Active Workers: 2\n",
      "\n",
      "Reading the variable \"Z1\" from \"cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr\"...\n",
      "Active Workers: 20\n",
      "Active Workers: 14\n",
      "Active Workers: 8\n",
      "Active Workers: 2\n",
      "Shutting down worker nodes...\n",
      "Workers shut down. (this may take a while to register in the platform UI)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nworkers</th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>resource</th>\n",
       "      <th>resource_csp</th>\n",
       "      <th>bucket</th>\n",
       "      <th>bucket_csp</th>\n",
       "      <th>fileFormat</th>\n",
       "      <th>original_dataset_name</th>\n",
       "      <th>data_variable</th>\n",
       "      <th>mem_bytes</th>\n",
       "      <th>cloud_bytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>read_time_seconds</th>\n",
       "      <th>throughput_MBps</th>\n",
       "      <th>throughput_std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF4</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>362236855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>3.487549</td>\n",
       "      <td>535.189383</td>\n",
       "      <td>98.212107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF4</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>362236855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>1.782855</td>\n",
       "      <td>1046.915614</td>\n",
       "      <td>208.670067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF4</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>362236855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>2.129929</td>\n",
       "      <td>876.320081</td>\n",
       "      <td>32.736628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>NetCDF4</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>362236855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>6.051105</td>\n",
       "      <td>308.455928</td>\n",
       "      <td>1.644334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>271346855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>0.980424</td>\n",
       "      <td>1903.768134</td>\n",
       "      <td>1582.484025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>271346855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>1.198084</td>\n",
       "      <td>1557.903434</td>\n",
       "      <td>816.141956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>271346855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>1.160945</td>\n",
       "      <td>1607.740719</td>\n",
       "      <td>91.064208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>gcptestnew</td>\n",
       "      <td>GCP</td>\n",
       "      <td>gs://cloud-data-benchmarks</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr</td>\n",
       "      <td>ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr</td>\n",
       "      <td>Z1</td>\n",
       "      <td>1866499208</td>\n",
       "      <td>271346855</td>\n",
       "      <td>134203968</td>\n",
       "      <td>3.471320</td>\n",
       "      <td>537.691560</td>\n",
       "      <td>3.248137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nworkers  nthreads  ncores    resource resource_csp  \\\n",
       "0        20        20      20  gcptestnew          GCP   \n",
       "1        14        14      14  gcptestnew          GCP   \n",
       "2         8         8       8  gcptestnew          GCP   \n",
       "3         2         2       2  gcptestnew          GCP   \n",
       "4        20        20      20  gcptestnew          GCP   \n",
       "5        14        14      14  gcptestnew          GCP   \n",
       "6         8         8       8  gcptestnew          GCP   \n",
       "7         2         2       2  gcptestnew          GCP   \n",
       "\n",
       "                       bucket bucket_csp fileFormat  \\\n",
       "0  gs://cloud-data-benchmarks        GCP    NetCDF4   \n",
       "1  gs://cloud-data-benchmarks        GCP    NetCDF4   \n",
       "2  gs://cloud-data-benchmarks        GCP    NetCDF4   \n",
       "3  gs://cloud-data-benchmarks        GCP    NetCDF4   \n",
       "4  gs://cloud-data-benchmarks        GCP       Zarr   \n",
       "5  gs://cloud-data-benchmarks        GCP       Zarr   \n",
       "6  gs://cloud-data-benchmarks        GCP       Zarr   \n",
       "7  gs://cloud-data-benchmarks        GCP       Zarr   \n",
       "\n",
       "                      original_dataset_name data_variable   mem_bytes  \\\n",
       "0                      ETOPO1_Ice_g_gmt4.nc            Z1  1866499208   \n",
       "1                      ETOPO1_Ice_g_gmt4.nc            Z1  1866499208   \n",
       "2                      ETOPO1_Ice_g_gmt4.nc            Z1  1866499208   \n",
       "3                      ETOPO1_Ice_g_gmt4.nc            Z1  1866499208   \n",
       "4  ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr            Z1  1866499208   \n",
       "5  ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr            Z1  1866499208   \n",
       "6  ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr            Z1  1866499208   \n",
       "7  ETOPO1_Ice_g_gmt4.nc_134MB_lz4_lvl5.zarr            Z1  1866499208   \n",
       "\n",
       "   cloud_bytes  chunksize  read_time_seconds  throughput_MBps  \\\n",
       "0    362236855  134203968           3.487549       535.189383   \n",
       "1    362236855  134203968           1.782855      1046.915614   \n",
       "2    362236855  134203968           2.129929       876.320081   \n",
       "3    362236855  134203968           6.051105       308.455928   \n",
       "4    271346855  134203968           0.980424      1903.768134   \n",
       "5    271346855  134203968           1.198084      1557.903434   \n",
       "6    271346855  134203968           1.160945      1607.740719   \n",
       "7    271346855  134203968           3.471320       537.691560   \n",
       "\n",
       "   throughput_std_dev  \n",
       "0           98.212107  \n",
       "1          208.670067  \n",
       "2           32.736628  \n",
       "3            1.644334  \n",
       "4         1582.484025  \n",
       "5          816.141956  \n",
       "6           91.064208  \n",
       "7            3.248137  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"ipynb_inputs.json\\\" \\\"read-data.py\\\" \\\"reads.csv\\\"\")\n",
    "df_reads = pd.read_csv(os.getcwd() + '/results/csv-files/reads.csv')\n",
    "df_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a169d8",
   "metadata": {},
   "source": [
    "## TODO: Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d69914",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fb9e7",
   "metadata": {},
   "source": [
    "## Step 5: Remove Benchmarking Files from Cloud Resources (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaca15a",
   "metadata": {},
   "source": [
    "Running the following cell will remove all files that the benchmark has copied/written to both clusters and cloud storage. It will not delete any of the original user-supplied datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9b9a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/0.0#1692395790148016...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/.zattrs#1692395785070895...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/.zgroup#1692395784544351...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/0.1#1692395791845648...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/1.0#1692395792047268...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/2.1#1692395792126024...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/.zmetadata#1692395786609167...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/2.0#1692395791552658...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/.zarray#1692395785804427...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/.zattrs#1692395786077701...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/5.0#1692395791843297...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/3.0#1692395792060513...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/3.1#1692395792124132...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/5.1#1692395792154284...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/6.0#1692395791995342...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/4.0#1692395792133255...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/6.1#1692395791748087...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/4.1#1692395791919028...\n",
      "Removing gs://cloud-data-benchmarks/cloud-data-transfer-benchmarking/cloudnativefiles/ETOPO1_Ice_g_gmt4.nc_134.203968MB_lz4_5.zarr/Z1/1.1#1692395791929584...\n",
      "/ [19/19 objects] 100% Done                                                     \n",
      "Operation completed over 19 objects.                                             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"bash postprocessing/remove-benchmark-files.sh \\\"ipynb_inputs.json\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
