{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06aad8f1",
   "metadata": {},
   "source": [
    "# Cloud Data Transfer Speeds Benchmarking Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551822",
   "metadata": {},
   "source": [
    "Add overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52c47",
   "metadata": {},
   "source": [
    "## Step 0: Load Required Setup Packages & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e427e0",
   "metadata": {},
   "source": [
    "Enter the following parameters to install packages to the correct conda instance and environment.\n",
    "\n",
    "\n",
    "`jupyter_conda_path : str`\n",
    "    - The path to miniconda that this Jupyter notebook is running from. Do not include a terminal `/` at the end of the path.\n",
    "    \n",
    "`jupyter_conda_env : str`\n",
    "    - The conda environment that this Jupyter notebook is running in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f93cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_conda_path = \"/home/jgreen/.miniconda3c\"\n",
    "jupyter_conda_env = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9c4fe",
   "metadata": {},
   "source": [
    "Installs required workflow setup packages and calls UI generation script. If one or more of the packages don't exist in the specified environment, they will install for you. Note that if installation is required, this cell will take a few minutes to complete execution.\n",
    "\n",
    "**NOTE: If you recieve an import error for `jupyter-ui-poll`, you will have to manually install the package in a user container terminal with the following commands:**\n",
    "```\n",
    "source <jupyter_conda_path>/etc/profile.d/conda.sh\n",
    "conda activate <jupyter_conda_env>\n",
    "pip install jupyter-ui-poll\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73185f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "print('Checking conda environment for UI depedencies...')\n",
    "os.system(\"bash \" + os.getcwd() + f\"/jupyter-helpers/install_ui_packages.sh {jupyter_conda_path} {jupyter_conda_env}\")\n",
    "print('All dependencies installed.')\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + '/jupyter-helpers')\n",
    "import ui_helpers as ui\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be35fd4",
   "metadata": {},
   "source": [
    "## Step 1: Define Workflow Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de7e39",
   "metadata": {},
   "source": [
    "Run the following cells to generate interactive widgets allowing you to enter all workflow inputs. **All inputs must be filled out to proceed with the benchmarking process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cb664",
   "metadata": {},
   "source": [
    "### Cloud Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7e51",
   "metadata": {},
   "source": [
    "#### Compute Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d055",
   "metadata": {},
   "source": [
    "Before defining anything else, the resources you intend to use with the benchmarking must be defined. Currently, only resources defined in the Parallel Works platform may be used. Also of note are options that will be passed to Dask: you must specify the number of cores and memory per worker node in the cluster. Without these values, Dask will not be able to submit jobs.\n",
    "\n",
    "In particular, these options are included so that you can form fair comparisons between different cloud service providers (CSPs). Generally, different CSPs won't have worker nodes with the exact same specs, and in order to achieve a fair comparison between two CSPs one cluster must be limited such that it does not exceed the computational power of the other.\n",
    "\n",
    "Before submitting your options, read and adhere to the following guidelines:\n",
    "\n",
    "- **When entering the number of CPUs in the following widget, you must know if the worker nodes of your clusters have hyperthreading enabled.** The instance type of a partition in the resource definition will display a number of vCPUs: this number represents the number of CPUs with hyperthreading enabled. **If hyperthreading is disabled, enter the number of vCPUs divided by 2 into this workflow.** If you do not know, assume that hyperthreading is disabled.\n",
    "\n",
    "- Supply a path to an existing miniconda installation or the desired installation path. Leave the default option of `~` to install to the home directory of the cluster. **Miniconda must be installed in a directory accessible to the head node and all worker nodes. If you are unsure if your current/desired installation directory is a shared directory, use the default option.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376e50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For resources controlled from the Parallel Works platform, the <code>Resource name</code> box should be populated with the name found on the <b>RESOURCES</b>  tab. Additional information such as memory and cores can be found from the respective resource definition.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21753c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resource = ui.resourceWidgets()\n",
    "resource.display()\n",
    "resources = resource.processInput()\n",
    "print(f'Your resource inputs:\\n {resources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4fe8c",
   "metadata": {},
   "source": [
    "#### Object Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea7fb6",
   "metadata": {},
   "source": [
    "This set of inputs is where you enter the cloud object store Universal Resource Identifiers (URIs). Both public and private buckets are supported. For the latter, ensure that you have access credentials with *at least* read, write, list, and put (copy from local storage to cloud) permissions, as format conversions will need to be made during the benchmarking process.\n",
    "\n",
    "Be sure to double-check your inputs to ensure that the bucket names and credentials are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b144152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "store = ui.storageWidgets()\n",
    "store.display()\n",
    "storage = store.processInput()\n",
    "\n",
    "# Following line should be commented out if you don't want AWS S3 credentials shown in plain text\n",
    "print(f'Your storage inputs:\\n {storage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711e2b",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323384",
   "metadata": {},
   "source": [
    "#### User-Supplied Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bd7e0",
   "metadata": {},
   "source": [
    "Below you can specify datasets that you want to be tested in the benchmarking. You can either enter single files or multiple files that belong to a single dataset, provided that the dataset matches one of the supported formats. **Read the following input rules after running the UI cell below this one.**\n",
    "\n",
    "\n",
    "1. Activate the checkbox if you desire to record your user-defined datasets. **If it is not checked, none of your inputs will be recorded.**\n",
    "\n",
    "<br>\n",
    "\n",
    "2. If speciying data in a bucket, input the path to the data within the bucket, *not* the full URI (i.e., use `path/to/file.extension` and not `<URI prefix>://<bucketname>/path/to/file.extension`)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Use absolute paths for data stored in the user container (or a filesystem mounted in the user container): `/path/to/data.extension`\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Use globstrings (`path/to/files/*`) to specify datasets that are split up into multiple subfiles.\n",
    "    - If using a globstring, ensure that *only* files that belong to the dataset exist in that directory. The workflow will take all files in the directory before the `*` and attempt to gather them into a single dataset.\n",
    "    - **Globstrings are NOT supported for NetCDF files**\n",
    "    \n",
    "<br>\n",
    "\n",
    "5. If you have a dataset stored in multiple cloud storage locations that will be used in the benchmarking, you must define an input for each of the locations. That is, you must define each location of the data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaed80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "userdata = ui.userdataWidgets(storage=storage)\n",
    "userdata.display()\n",
    "user_files = userdata.processInput()\n",
    "print(f'Your data inputs:\\n {user_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c5eb",
   "metadata": {},
   "source": [
    "#### Randomly-Generated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322692c1",
   "metadata": {},
   "source": [
    "Another option to supply data to the benchmarking is to create randomly-generated datasets. CSV datasets can be as large as you want and provide a great option if you are new to the world of cloud-native data formats. There are currently two supported randomly-generated data formats: CSV and NetCDF4. Since NetCDF4 is a gridded data format, options to specify the number of coordinate axes and data variables are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf879954",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Randomly-generated NetCDF4 file sizes are limited by available disk space in the cluster you are generating the file with. Ensure that you have adequate disk space in your cluster, or the file will not fully generate.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46825cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "randgen = ui.randgenWidgets(resources=resources)\n",
    "randgen.display()\n",
    "randfiles = randgen.processInput()\n",
    "print(f'Your randomly-generated file options:\\n {randfiles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765ab83",
   "metadata": {},
   "source": [
    "### Benchmark Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfffa69",
   "metadata": {},
   "source": [
    "#### Legacy to Cloud-Native Conversion Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfdd5e",
   "metadata": {},
   "source": [
    "There are a number of different options to choose from when legacy files (e.g., CSV, NetCDF) are converted to cloud-native (e.g., Parquet, Zarr). In fact, this input field is where you have the most control over the type of resutls you want to see. You may specify different compression algorithms, compression levels, and chunksizes to use when cloud-native files are written to cloud storage, which each have different effects on how fast files will be written and read.\n",
    "\n",
    "Note that, for large chunksizes, the workflow may fail. This is an intended consequence, as the workflow is designed to let you explore the effects of different options on your data. **In general, it is recommended that you keep chunksizes in the range of 50-150 megabytes**. Anything higher will result in either poor performance or out-of-memory errors. Similarly, some compression algorithms will result in poor performance.\n",
    "\n",
    "The widget below allows you to define different sets of options to convert files with. To make multiple selections in the compression algorithm and dataset fields, simply hold `SHIFT` and/or `CTRL` (or `COMMAND` on Mac). Using these option sets, the files defined in each set will be converted using the specified compression & chunksize information. The following rules apply:\n",
    "\n",
    "- An individual dataset will be converted `(# of compression algorithms in option set 1)*...*(# of compression algorithms in option set n)` times\n",
    "- The total number of conversions is equal to the sum of individual dataset conversions\n",
    "- Datasets will only be converted if they are selected in at least one option set.\n",
    "- **If you want to keep the internal chunking scheme of a NetCDF4 dataset, set chunksize to 0**\n",
    "\n",
    "Gridded data chunk dimensions are computed as n-dimensional cubes (i.e., a data variable described by 3 dimensions will have a 3-dimensional chunk of uniform dimension lengths, and so on for higher-order cases). This may give suboptimal chunk sizes across the entire dataset (i.e., there may be more \"filler\" chunks: chunks that are used to fill the spaces where desired chunk dimensions cannot fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertOptions = ui.convertOptions(user_files, randfiles)\n",
    "convertOptions.display()\n",
    "convert_options = convertOptions.processInput()\n",
    "print(f'Your cloud-native data options:\\n {convert_options}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa84e5",
   "metadata": {},
   "source": [
    "#### Core Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303ad9f",
   "metadata": {},
   "source": [
    "Finally, there are a couple important options that apply to the core of the benchmarks. These are:\n",
    "\n",
    "- `worker_step : int` - When performing file reads (option not yet implemented in conversions), the workflow will loop through a range of Dask workers (or number of parallel reads). The loop starts at the maximum defined worker amount, and reduces by `worker_step` until the lowest possible value of workers is reached.\n",
    "\n",
    "</br>\n",
    "\n",
    "- `tests : int` - The number of times each individual file will be read for measurement. Entering a number greater than 1 will take much longer to run, but results will reflect the volaility in network performance you can expect when using the data for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84346150",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_step = 5\n",
    "tests = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d585b",
   "metadata": {},
   "source": [
    "## Step 2: Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f2c55",
   "metadata": {},
   "source": [
    "Executing the following cell will write all of your inputs to `inputs.json`, install miniconda3 and the \"cloud-data\" Python environment to all resources, and write randomly-generated files to all cloud storage locations (if any files were specified). If writing randomly-generated files, especially large ones, the execution of this cell may take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb642c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "While randomly-generated files are written in parallel by default, if you wish to speed up the execution of this cell, consider creating/choosing a resource with more powerful worker nodes.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff33e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Setting up workflow...')\n",
    "\n",
    "user_input = json.dumps({\"RESOURCES\" : resources,\n",
    "                         \"STORAGE\" : storage,\n",
    "                         \"USERFILES\" : user_files,\n",
    "                         \"RANDFILES\" : randfiles,\n",
    "                         \"CONVERTOPTS\" : convert_options,\n",
    "                         \"GLOBALOPTS\" : {'worker_step' : worker_step, 'tests' : tests}\n",
    "                        })\n",
    "\n",
    "with open('inputs.json', 'w') as outfile:\n",
    "    outfile.write(user_input)\n",
    "\n",
    "os.system(\"bash workflow_notebook_setup.sh\")\n",
    "\n",
    "print('Workflow setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ea9b",
   "metadata": {},
   "source": [
    "## Step 3: Run Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d260",
   "metadata": {},
   "source": [
    "### Convert File to Cloud-Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad150c9",
   "metadata": {},
   "source": [
    "Since one of the major goals of this benchmarking is testing legacy formats against cloud-native ones, we must convert your legacy-formatted data (CSV and NetCDF4) into their corresponding cloud-native formats. This cell will execute and time the conversion process, writing each new format in parallel. The conversion will be done using each cluster's full amount of resources, so be mindful of this feature when using clusters that are expensive to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4685034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"convert-data.py\\\" \\\"conversions.csv\\\"\")\n",
    "df_conversions = pd.read_csv(os.getcwd() + '/results/csv-files/conversions.csv')\n",
    "df_conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702d935",
   "metadata": {},
   "source": [
    "### File Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffc4ec",
   "metadata": {},
   "source": [
    "The last computation-intensive test in the benchmarking is reading and timing files from cloud storage. This will give you an idea of what data transfer throughput you can expect when using cloud storage and different data formats in other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"bash benchmarks-core/run_benchmark_step.sh \\\"read-data.py\\\" \\\"reads.csv\\\"\")\n",
    "df_reads = pd.read_csv(os.getcwd() + '/results/csv-files/reads.csv')\n",
    "df_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a169d8",
   "metadata": {},
   "source": [
    "## TODO: Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d69914",
   "metadata": {},
   "source": [
    "**Feature not ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fb9e7",
   "metadata": {},
   "source": [
    "## Step 5: Remove Benchmarking Files from Cloud Resources (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ff657",
   "metadata": {},
   "source": [
    "Running the following cell will remove all files that the benchmark has copied/written to both clusters and cloud storage. It will not delete any of the original user-supplied datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"bash postprocessing/remove-benchmark-files.sh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
